# Руководство по оптимизации для больших файлов

## Проблема
При обработке файлов с более чем 200 артикулами парсер потребляет слишком много ресурсов (CPU и память), что приводит к зависанию сервера.

## Внесенные оптимизации

### 1. Оптимизация парсера (`backend/api/tasks.py`)
- **Уменьшено количество потоков**: с 8+10 до 3+2
- **Добавлены задержки**: 0.1с между запросами Autopiter/Emex, 0.2с для Selenium
- **Улучшена обработка ошибок**: таймауты и try-catch блоки
- **Оптимизация памяти**: принудительная очистка памяти каждые 10 строк
- **Ограничение логов**: только последние 100-500 строк

### 2. Оптимизация парсеров (`backend/api/autopiter_parser.py`)
- **Уменьшены таймауты**: с 15с до 10с для HTTP, с 30с до 20с для Selenium
- **Оптимизация Selenium**: отключение GPU, расширений, изображений
- **Уменьшение размера окна**: с 1920x1080 до 1280x720
- **Сокращение попыток**: с 3 до 2 для HTTP запросов

### 3. Оптимизация Celery (`backend/backend/settings.py`)
- **Один воркер**: `CELERY_WORKER_CONCURRENCY = 1`
- **Ограничение задач**: `CELERY_WORKER_MAX_TASKS_PER_CHILD = 10`
- **Отключение событий**: `CELERY_WORKER_SEND_TASK_EVENTS = False`
- **Игнорирование результатов**: `CELERY_TASK_IGNORE_RESULT = True`

### 4. Ограничения ресурсов в Docker
```yaml
celery:
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '0.5'
      reservations:
        memory: 512M
        cpus: '0.25'
```

### 5. Мониторинг ресурсов (`backend/monitor_resources.py`)
- Автоматический перезапуск Celery при превышении лимитов
- Мониторинг использования памяти
- Логирование состояния ресурсов

## Рекомендации по серверу

### Текущая конфигурация (недостаточная)
- **CPU**: 1 ядро
- **Память**: 2 ГБ
- **Проблема**: Недостаточно ресурсов для обработки больших файлов

### Рекомендуемая конфигурация
- **CPU**: 2-4 ядра
- **Память**: 4-8 ГБ
- **Диск**: SSD для быстрого I/O

### Минимальная рабочая конфигурация
- **CPU**: 2 ядра
- **Память**: 4 ГБ
- **Примечание**: Обработка будет медленнее, но стабильной

## Как запустить оптимизированную версию

### 1. Перезапуск с новыми настройками
```bash
docker compose down
docker compose up -d
```

### 2. Запуск мониторинга ресурсов (опционально)
```bash
cd backend
python monitor_resources.py
```

### 3. Проверка логов
```bash
docker compose logs celery
```

## Ожидаемые результаты

### До оптимизации
- Парсер зависал при 200+ артикулах
- Потребление памяти: 1.5-2 ГБ
- CPU: 90-100%

### После оптимизации
- Стабильная работа с 200+ артикулами
- Потребление памяти: 500-800 МБ
- CPU: 50-70%
- Время обработки: увеличится в 2-3 раза, но стабильно

## Дополнительные рекомендации

### Для очень больших файлов (500+ артикулов)
1. **Разбивайте файлы**: Обрабатывайте по 100-200 артикулов за раз
2. **Используйте очередь**: Загружайте несколько файлов последовательно
3. **Мониторинг**: Следите за логами и ресурсами

### Настройка для продакшена
1. **Увеличьте ресурсы сервера** до рекомендуемой конфигурации
2. **Настройте мониторинг** с автоматическими уведомлениями
3. **Добавьте кеширование** результатов парсинга
4. **Используйте CDN** для статических файлов

## Устранение проблем

### Если парсер все еще зависает
1. Проверьте логи: `docker compose logs celery`
2. Увеличьте ресурсы сервера
3. Разбейте файл на меньшие части
4. Запустите мониторинг ресурсов

### Если медленная обработка
1. Это нормально после оптимизации
2. Увеличьте количество потоков (но не более 2-3)
3. Рассмотрите возможность увеличения ресурсов сервера 